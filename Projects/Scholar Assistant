Introduction

  There are possibilities to introduce AI Machine Learning into creating and validating scientific publications with ease and accuracy. A similar idea is found in mathematics
  proof assistants that validate the logical validity of a step by step proof work.


Given Concepts

  A scientific publication, at least to this topic's interest, follows this structure:
    -- References/Axyoms/Definitions;
    -> Reasoning + Research;
    -> Conclusion.

  The linking of references between publications create an academic publishing network/tree.

  A token is a proof of validity that assures a given article has accurate and truthful sources, verified by specialists and assistants.


Idea

  The suggestion for the assistant is the introduction of softwares and AI to create consistent links between references and conclusions, while providing validity tokens, which can
  be generated via plugin from 3rd party APIs and be used as a sign of a well developed research.


Broad Definitions

  Concerning a publication:
  
    -[Black Paper]:  with researches and conclusions that violate ethics and laws
    -[Red Paper]:    one publication which is proven incorrect
    -[Yellow Paper]: ongoing research / incomplete conclusion / dubious tokens / unprecise usage of definitions / inaccurate
    -[Green Paper]:  a publication or article with all steps verified with tokens and a conclusion which is accurate and precise


  A network/tree consisting of publications can generate new research and articles have the following rules:
  
    -[Green Paper] can only be generated by [Green Paper];
    -[Yellow Paper] can be generated only by [Green Paper, Yellow Paper, Black Paper];
    -[Red Paper] can be generated by any of the categories;
    -[Black Paper] can be generated by any of the categories.


  Concerning a Token:
  
    A token can be produced by:
      -Experts, with the use of a [Green Paper]
      -APIs, with enough arbitrary accuracy/precision. The accuracy/precision is to be defined later, while staying as close to 100% as possible.
    
    Tokens do not have categories for the purpose of rigorous validity: they are always "Green".
    
    
Practice
  
  Definitions:
  
    [Logic] : Logical operators:
      [if], [then], etc..
      
    {Proposition} : A statement.
      {i exist}, {1 = 0}, {mathematics is cool}, etc..
      A proposition can be linked with other propositions.
      
    R[Reference] : to include references or research data.
      R[according], R[shows], R[referencing], etc..
      
    R{Reference Name} : state the reference address.
      R{Newton}, R{Marx}, etc..
    
    D{Definition} : defining a statement.
      D{i am cool}, etc...
    
  The logical steps could be verified with expert assistants or Machine Learning assistants. 
    Examples: 
      "R[according to] R{newton}, [if] {{F = m.a}{m = 10}{a = 2}}, [then] {F = 20}";
      "As R[stated] by R{pythagoras}, {a2 + b2 = c2}";
      "We define evolution stages as D{EvoStage; stages that follow these rules...}, [implying that] {humanity is on stage 2.5 of evolution} R[according to] R{EvoStage}".
      
  Such logical skeleton would be hidden for casual reading through articles.
  
    
Conclusion

  This system is an approach to scientific research and the hunt for the spread of "Fake News". Publication, articles and news must possess [Green Papers] and validity Tokens
  to be deemed truthful. Or based in verified sources at last.
